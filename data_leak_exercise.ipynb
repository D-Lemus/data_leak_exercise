{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<span style=\"color: rgb(0,53,91);\">\n",
    "<center><img src=\"https://imgs.search.brave.com/vIujYFx1qV5CoxJb72HNVYIoBXQjuasIvdngB1DPC3s/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly91cGxv/YWQud2lraW1lZGlh/Lm9yZy93aWtpcGVk/aWEvZW4vdGh1bWIv/NS81Zi9XZXN0ZXJu/X0luc3RpdHV0ZV9v/Zl9UZWNobm9sb2d5/X2FuZF9IaWdoZXJf/RWR1Y2F0aW9uX2xv/Z28ucG5nLzIwMHB4/LVdlc3Rlcm5fSW5z/dGl0dXRlX29mX1Rl/Y2hub2xvZ3lfYW5k/X0hpZ2hlcl9FZHVj/YXRpb25fbG9nby5w/bmc\" style=\"width:180px;height:142px;\" title=\"Logo ITESO\"></center>\n",
    "<font face = \"Times New Roman\" size = \"6\"><b><center>Programacion para Mineria de Datos</center></b></font>\n",
    "<font face = \"Times New Roman\" size = \"5\"><b><center>TAREA: Fuga de Datos</center></b></font>\n",
    "\n",
    "<b><font back = \"Times New Roman\" size = \"4\"><center>Unidad 4: </center></font>\n",
    "<font face = \"Times New Roman\" size = \"4\"><center>Tema 4.3: Integración de Datos: Categóricos, Numéricos, Faltantes y Fuga de Datos</center></font>\n",
    "\n",
    "<b><font back = \"Times New Roman\" size = \"4\"><center>Equipo: </center></font>\n",
    "<font face = \"Times New Roman\" size = \"4\"><center> Diego Lemus, Sebastian Zinchenko, Jersus Vargas </center></font>\n",
    "\n",
    "<div align=\"right\"><font face = \"Times New Roman\" size = \"2\">Mtro. Miguel Tlapa Juarez (migueltlapa@iteso.mx)</font></div>\n",
    "</span></div>\n",
    "\n",
    "\n"
   ],
   "id": "82b7b80834df08be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import RandomState\n",
    "from pandas.core.common import random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from unicodedata import numeric\n",
    "\n"
   ],
   "id": "f30269d501fe4125"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cargamos CSV",
   "id": "8089246173ad7f7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = \"data/datos_processing_integration.csv\"\n",
    "df = pd.read_csv(data)\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "id": "900147c01a82716b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Splitting",
   "id": "df58c3a7bc2d5f2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Traning environment with a random splitting and then shuffle them\n",
    "X_train, X_test = train_test_split(df, test_size=0.20, random_state=12, shuffle=True)\n",
    "\n",
    "#Including numbers from our new list\n",
    "num_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "#Excluding numbers from our new list\n",
    "cat_cols = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"num: {num_cols}\")\n",
    "print(f\"categorical: {cat_cols}\")"
   ],
   "id": "352c24d59bf5a3b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Histograma de dispersion\n",
   "id": "ad09db1abccb5ab8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "columns = ['edad','ingreso_mensual','talla_cm','peso_kg','temperatura_c','frecuencia_cardiaca']\n",
    "def hist_dist_normal(df, column):\n",
    "\n",
    "    escalado = df[column]\n",
    "\n",
    "    media = np.mean(escalado)\n",
    "    desviacion = np.std(escalado)\n",
    "    print(media)\n",
    "    print(desviacion)\n",
    "\n",
    "    x = np.linspace(media - 3*desviacion, media + 3*desviacion, 100)\n",
    "    y = norm.pdf(x, media, desviacion)\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot(x, y, color='red', label='Distribución Normal')\n",
    "    plt.fill_between(x, y, color='red', alpha=0.2)\n",
    "\n",
    "    sns.histplot(escalado, kde=False, stat='density', bins=10, color='blue', edgecolor='white', label='Datos reales')\n",
    "\n",
    "    plt.title('Campana de Gauss', fontsize=16)\n",
    "    plt.xlabel('Valor {}'.format(column), fontsize=14)\n",
    "    plt.ylabel('Densidad', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "for column in columns:\n",
    "    hist_dist_normal(X_train,column)"
   ],
   "id": "2ff922fdf49d453"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Deteccion de Outliers",
   "id": "55561eff219e4258"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def outliers_summary(df_in: pd.DataFrame, numeric_cols):\n",
    "    rows = []\n",
    "    for col in numeric_cols:\n",
    "        s_raw = pd.to_numeric(df_in[col], errors='coerce')\n",
    "        s = s_raw.dropna()\n",
    "\n",
    "        #formato empty para mostrar lo que se necesita para aplicar la heuristica\n",
    "        if s.empty:\n",
    "            rows.append({\n",
    "                \"outliers_n\": 0, \"outliers_%\": 0.0, \"|skew|\": np.nan,\n",
    "                \"recom_imputer\": \"median\", \"imputer_to_use\": \"RobustScaler\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "        #rango intercuartilar\n",
    "        iqr = q3 - q1\n",
    "        #limites\n",
    "        lim_inf, lim_sup = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "        #mascara para contar los outliers\n",
    "        mask = (s < lim_inf) | (s > lim_sup)\n",
    "        out_n = int(mask.sum())\n",
    "        out_pct = 100.0 * out_n / len(s)\n",
    "\n",
    "        skew_abs = float(abs(s.skew()))\n",
    "\n",
    "        # Heurística de IMPUTACIÓN\n",
    "        imputer = \"median\" if (out_pct >= 5.0 or skew_abs >= 1.0) else \"mean\"\n",
    "        # Heurística de ESCALADO\n",
    "        if out_pct >= 5.0:\n",
    "            scaler = \"RobustScaler\"\n",
    "        elif skew_abs <= 0.5:\n",
    "            scaler = \"StandardScaler\"\n",
    "        else:\n",
    "            scaler = \"MinMaxScaler\"\n",
    "\n",
    "        rows.append({\n",
    "            \"| columna |\": col,\n",
    "            \"| outliers_n |\": out_n, \"outliers_%\": round(out_pct, 2),\n",
    "            \"| skew |\": round(skew_abs, 3),\n",
    "            \"imputer_to_use\": imputer,\n",
    "            \"scaler_to_use\": scaler\n",
    "        })\n",
    "\n",
    "    rep = pd.DataFrame(rows).sort_values(\"outliers_%\", ascending=False).reset_index(drop=True)\n",
    "    return rep\n",
    "\n",
    "\n",
    "print(outliers_summary(X_train,num_cols))"
   ],
   "id": "e5102e87b7c8a64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# g) Selección manual de columnas para StandardScaler",
   "id": "9a49b67b60002058"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Según el análisis de outliers y la heurística:\n",
    "# StandardScaler se aplica a columnas con outliers < 5% y |skew| <= 0.5\n",
    "\n",
    "standard_scaler_cols = [\n",
    "    'edad',                  # 0% outliers, skew=0.259\n",
    "    'ingreso_mensual',       # 1.35% outliers, skew=0.114\n",
    "    'talla_cm',              # 0% outliers, skew=0.007\n",
    "    'peso_kg',               # 0% outliers, skew=0.185\n",
    "    'temperatura_c',         # 4.05% outliers, skew=0.312\n",
    "    'frecuencia_cardiaca'    # 0% outliers, skew=0.082\n",
    "]\n",
    "\n",
    "print(\"Columnas seleccionadas para StandardScaler:\")\n",
    "print(standard_scaler_cols)"
   ],
   "id": "8e2aead025a40768"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# h) Selección manual de columnas para MinMaxScaler",
   "id": "3521eb18c7d22d59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Según el análisis de outliers y la heurística:\n",
    "# MinMaxScaler se aplica cuando no hay muchos outliers pero |skew| > 0.5\n",
    "\n",
    "minmax_scaler_cols = [\n",
    "    'sensor_defectuoso'  # 0% outliers, skew=0.734 (entre 0.5 y 1.0)\n",
    "]\n",
    "\n",
    "print(\"Columnas seleccionadas para MinMaxScaler:\")\n",
    "print(minmax_scaler_cols)"
   ],
   "id": "cc0f8b6fe9a2eb49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# i) Aplicar escalamiento StandardScaler",
   "id": "5c44570a685a3132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Aplicar StandardScaler a las columnas seleccionadas\n",
    "# IMPORTANTE: fit() solo con X_train para evitar fuga de datos\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "\n",
    "# Ajustar el scaler SOLO con datos de entrenamiento\n",
    "scaler_standard.fit(X_train[standard_scaler_cols])\n",
    "\n",
    "# Transformar tanto train como test\n",
    "X_train_standard_scaled = scaler_standard.transform(X_train[standard_scaler_cols])\n",
    "X_test_standard_scaled = scaler_standard.transform(X_test[standard_scaler_cols])\n",
    "\n",
    "# Convertir a DataFrame para facilitar manipulación\n",
    "X_train_standard_df = pd.DataFrame(\n",
    "    X_train_standard_scaled,\n",
    "    columns=standard_scaler_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_standard_df = pd.DataFrame(\n",
    "    X_test_standard_scaled,\n",
    "    columns=standard_scaler_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(\"StandardScaler aplicado correctamente.\")\n",
    "print(f\"Shape X_train escalado: {X_train_standard_df.shape}\")\n",
    "print(f\"Shape X_test escalado: {X_test_standard_df.shape}\")\n",
    "print(\"\\nPrimeras filas de X_train escalado:\")\n",
    "print(X_train_standard_df.head())"
   ],
   "id": "eba40e0c9cb95ae8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# j) Aplicar escalamiento MinMaxScaler",
   "id": "a0e82122a9387495"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Aplicar MinMaxScaler a las columnas seleccionadas\n",
    "# IMPORTANTE: fit() solo con X_train para evitar fuga de datos\n",
    "\n",
    "scaler_minmax = MinMaxScaler()\n",
    "\n",
    "# Ajustar el scaler SOLO con datos de entrenamiento\n",
    "scaler_minmax.fit(X_train[minmax_scaler_cols])\n",
    "\n",
    "# Transformar tanto train como test\n",
    "X_train_minmax_scaled = scaler_minmax.transform(X_train[minmax_scaler_cols])\n",
    "X_test_minmax_scaled = scaler_minmax.transform(X_test[minmax_scaler_cols])\n",
    "\n",
    "# Convertir a DataFrame para facilitar manipulación\n",
    "X_train_minmax_df = pd.DataFrame(\n",
    "    X_train_minmax_scaled,\n",
    "    columns=minmax_scaler_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_minmax_df = pd.DataFrame(\n",
    "    X_test_minmax_scaled,\n",
    "    columns=minmax_scaler_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(\"MinMaxScaler aplicado correctamente.\")\n",
    "print(f\"Shape X_train escalado: {X_train_minmax_df.shape}\")\n",
    "print(f\"Shape X_test escalado: {X_test_minmax_df.shape}\")\n",
    "print(\"\\nPrimeras filas de X_train escalado:\")\n",
    "print(X_train_minmax_df.head())"
   ],
   "id": "c68d05f552c3e23f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Guardar datos numéricos escalados en CSV",
   "id": "99e2afe86ff092eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Combinar todas las columnas numéricas escaladas\n",
    "X_train_scaled = pd.concat([X_train_standard_df, X_train_minmax_df], axis=1)\n",
    "X_test_scaled = pd.concat([X_test_standard_df, X_test_minmax_df], axis=1)\n",
    "\n",
    "# Combinar train y test para guardar todo junto\n",
    "X_scaled_completo = pd.concat([X_train_scaled, X_test_scaled], axis=0)\n",
    "\n",
    "# Guardar en CSV\n",
    "X_scaled_completo.to_csv('data/datos_numericos_escalados.csv', index=True)\n",
    "\n",
    "print(\"Datos numéricos escalados guardados exitosamente!\")\n",
    "print(f\"Shape del dataset completo escalado: {X_scaled_completo.shape}\")\n",
    "print(f\"\\nColumnas guardadas:\")\n",
    "print(X_scaled_completo.columns.tolist())\n",
    "print(f\"\\nPrimeras filas del CSV:\")\n",
    "print(X_scaled_completo.head())"
   ],
   "id": "cd81281ae1678b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Categorizacion",
   "id": "f134248ff4369250"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Nominales (One-Hot). Uso cat cols a excepcion de nivel\n",
    "onehot_cols = [c for c in cat_cols if c != \"nivel\"]\n",
    "\n",
    "# Ordinales\n",
    "ordinal_cols = [\"nivel\"]\n",
    "ordinal_categories = {\"nivel\": [\"bajo\",\"medio\",\"alto\"]}"
   ],
   "id": "45bc5fdd2c867409"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imputacion de categóricas",
   "id": "95058d4dd83627f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cat_imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "# categorical imputer fit - aprende cual es la moda en X train, solo lo haces con train para\n",
    "# rellenar los valores faltantes\n",
    "cat_imp.fit(X_train[onehot_cols + ordinal_cols])\n",
    "\n",
    "#datafranes sin nulos\n",
    "Xtr_cat = pd.DataFrame(\n",
    "    cat_imp.transform(X_train[onehot_cols + ordinal_cols]),\n",
    "    columns=onehot_cols + ordinal_cols, index=X_train.index\n",
    ")\n",
    "\n",
    "Xte_cat = pd.DataFrame(\n",
    "    cat_imp.transform(X_test[onehot_cols + ordinal_cols]),\n",
    "    columns=onehot_cols + ordinal_cols, index=X_test.index\n",
    ")"
   ],
   "id": "d40973c399a5572d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ordinal",
   "id": "5b9844023feb4a86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordenc = OrdinalEncoder(\n",
    "    categories=[ordinal_categories[\"nivel\"]],\n",
    "    # handle_unknown=\"use_encoded_value\",\n",
    "    #vimos en clase que si hay una categoria vista se le asigne -1\n",
    "    unknown_value=-1\n",
    ")\n",
    "ordenc.fit(Xtr_cat[ordinal_cols])  # <-- SOLO TRAIN (sin fuga)\n",
    "\n",
    "#DataFrame de Ordinal para entrenamiento (X Train Ordinal)\n",
    "Xtr_ord = pd.DataFrame(\n",
    "    ordenc.transform(Xtr_cat[ordinal_cols]),\n",
    "    columns=[f\"ORD_{c}\" for c in ordinal_cols],\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "#DataFrame de Ordinal para testing (X test Ordinal)\n",
    "Xte_ord = pd.DataFrame(\n",
    "    ordenc.transform(Xte_cat[ordinal_cols]),\n",
    "    columns=[f\"ORD_{c}\" for c in ordinal_cols],\n",
    "    index=X_test.index\n",
    ")"
   ],
   "id": "4ea2325bf2eea68a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# One-Hot",
   "id": "fead3196d039fe6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# ONE-HOT\n",
    "if onehot_cols:\n",
    "\n",
    "    #aplicamos on hot ignorando los unknowns\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    ohe.fit(Xtr_cat[onehot_cols])  # <-- SOLO TRAIN (sin fuga)\n",
    "\n",
    "    #One-Hot encoder data frame de entrenamiento (X Test One.Hot.Encoder)\n",
    "    Xtr_ohe = pd.DataFrame(\n",
    "        ohe.transform(Xtr_cat[onehot_cols]),\n",
    "        #sacamos los titulos\n",
    "        columns=ohe.get_feature_names_out(onehot_cols),\n",
    "        index=X_train.index\n",
    "    )\n",
    "\n",
    "    #One-Hot encoder data frame de testing (X Test One.Hot.Encoder)\n",
    "    Xte_ohe = pd.DataFrame(\n",
    "        ohe.transform(Xte_cat[onehot_cols]),\n",
    "        #sacamos los titulos\n",
    "        columns=ohe.get_feature_names_out(onehot_cols),\n",
    "        index=X_test.index\n",
    "    )\n",
    "else:\n",
    "    Xtr_ohe = pd.DataFrame(index=X_train.index)\n",
    "    Xte_ohe = pd.DataFrame(index=X_test.index)"
   ],
   "id": "1013c4e9769a668f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reconstruccion",
   "id": "3a00952a5d83de2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Creamos dataset exclusivamente para los datos categoricos cosificados\n",
    "X_train_cat_only = pd.concat([Xtr_ohe.reset_index(drop=True),\n",
    "                              Xtr_ord.reset_index(drop=True)], axis=1)\n",
    "X_test_cat_only  = pd.concat([Xte_ohe.reset_index(drop=True),\n",
    "                              Xte_ord.reset_index(drop=True)], axis=1)\n",
    "\n",
    "X_train_cat_only.to_csv(f\"data/Solo_Categoricos_train.csv\")\n",
    "X_test_cat_only.to_csv(\"data/Solo_Categoricos_test.csv\")"
   ],
   "id": "d68d79609046a8d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Consluiones",
   "id": "78ca86b5e9cc9f3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Diego Lemus**:\n",
    "#### Realicé un preprocesamiento sistemático para asegurar la calidad e integridad de los datos: separé primero en entrenamiento y prueba para evitar fuga, analicé distribución y atípicos, y apliqué una heurística coherente para imputar y escalar. Las variables categóricas se codificaron de forma adecuada (ordinal y one-hot) y todo el pipeline se ajustó solo con entrenamiento y luego se aplicó a prueba. Con ello obtuve un dataset consistente, limpio y reproducible, listo para cualquier modelo posterior.\n",
    "\n",
    "### **Sebastien Zinchenko**:\n",
    "#### Este ejercicio me enseñó que preparar datos correctamente requiere dividir train/test antes de cualquier transformación para evitar contaminar los resultados, y que cada columna necesita un tratamiento específico según sus características. Lo más valioso fue aprender a observar los datos, tomar decisiones fundamentadas con evidencia del análisis, y entender que el preprocesamiento no es aplicar recetas sino interpretar lo que los datos realmente necesitan.\n",
    "\n",
    "### **Jesus Vargas**:\n",
    "#### A lo largo de este ejercicio se aplicaron diversas técnicas fundamentales del preprocesamiento de datos, las cuales son esenciales en la minería de datos y el aprendizaje automático. Se comprendió la importancia de limpiar, escalar y analizar la calidad del dataset antes de usarlo en cualquier modelo predictivo."
   ],
   "id": "1744698332ec7c23"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
